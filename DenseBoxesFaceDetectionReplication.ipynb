{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan\n",
    "* Sit for 8 hours and finish it. Rest will happen automatically. \n",
    "* Function call flow:\n",
    "* x -> x_down -> y_loc, y_class -> pred_loc, pred_class -> loss_class, loss_reg -> loss\n",
    "* This completes the feedback loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes\n",
    "The following classes/methods exist in the dataset\n",
    "* downsample\n",
    "* encode\n",
    "* Network\n",
    "* Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_y(x_down, labels):\n",
    "    \"\"\"\n",
    "    x_downsampled tensor -> y_tensor: Numpy\n",
    "    \n",
    "    Args:\n",
    "        x_down (b, 3, 60, 60): Downsampled list of images\n",
    "        labels_down (b, v, 2): List of lists of lists (downsampled boxes)\n",
    "        \n",
    "    Returns:\n",
    "        y_class (b, 1, 60, 60): Tensor containing mask for each image. Calculated within encode\n",
    "        y_loc (b, 4, 60, 60):\n",
    "    \"\"\"\n",
    "    \n",
    "    y_class = np.zeros((x_down.shape[0], x_down.shape[2], x_down.shape[3]))\n",
    "    y_loc = np.zeros((x_down.shape[0], 4, x_down.shape[2], x_down.shape[3]))\n",
    "    \n",
    "    y_class[x_down[:,0,:,:]>0] = 1 # Can also choose a smaller neighbourhood here\n",
    "    pos_inds = np.argwhere(y_class) # y_class is also the positive examples mask\n",
    "    \n",
    "    for b, y, x in pos_inds:\n",
    "        y_loc[b, :, y, x] = match_boxes(x, y, labels_down[b])\n",
    "        \n",
    "    y_class = y_class.reshape(x_down.shape[0], 1, x_down.shape[2], x_down.shape[3])\n",
    "    \n",
    "    return y_class, y_loc\n",
    "\n",
    "def match_boxes(x, y, boxes):\n",
    "    \"\"\" Numpy\n",
    "    Matches a point (x,y) to a bunch of boxes. Returns offset of the box with the nearest centre.\n",
    "    \n",
    "    Args:\n",
    "        x (scalar): X coordinate of point being matched\n",
    "        y (scalar): Y coordinate of point being matched\n",
    "        boxes: List of list of downsampled boxes being matched, in (tx, ty, bx, by) notation\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    dist = 10**5 # Store the smallest disance to a large no initially, can glitch if dist greater than this\n",
    "    \n",
    "    for box in boxes:\n",
    "        cx = (box[0] + box[2])/2\n",
    "        cy = (box[1] + box[3])/2\n",
    "        \n",
    "        box_dist = (cx - x)**2 + (cy - y)**2\n",
    "        \n",
    "        if box_dist < dist:\n",
    "            offset = np.array([box[0] - x, box[1] - y, box[2] - x, box[3] - y])\n",
    "            dist = box_dist\n",
    "            \n",
    "    # Should not glitch because matching is only done for positive indices\n",
    "    return offset\n",
    "    \n",
    "\n",
    "def downsample(x):\n",
    "    \"\"\"\n",
    "    x -> x/4\n",
    "    \n",
    "    Args:\n",
    "        x (b, 3, 240, 240): Batch of 3 channel 240 x 240 images\n",
    "\n",
    "    Returns:\n",
    "        x_down (b, 3, 60, 60): Batch of 3 channel 60x60 images\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def loss(pred_class, pred_loc, gt_class, gt_loc, lambd=0.4):\n",
    "    \"\"\"\n",
    "    Calculates weighted sum of classification and regression loss. Calls the classification loss and regression loss functions separately.\n",
    "    \n",
    "    Args:\n",
    "        pred_class (b, 1, 60, 60): Network confidence probs for images\n",
    "        pred_loc (b, 4, 60, 60): Network offsets for each location\n",
    "        gt_class (b, 1, 60, 60): Gt class scores from encode\n",
    "        gt_loc (b, 4, 60, 60): Gt regression offsets from encode\n",
    "        lambd (scalar): WEighting factor comparison regression loss to \n",
    "        \n",
    "    Returns:\n",
    "        loss: Scalar value of \n",
    "    \"\"\"\n",
    "    return classification_loss(pred_class, gt_class) + lambd * regression_loss(pred_loc, gt_loc, gt_class)\n",
    "\n",
    "def classification_loss(pred_class, gt_class):\n",
    "    \"\"\"\n",
    "    Classification loss from mean squared diff between probabilities. Should probably use cross entropy instead but usng this now for simplicity.\n",
    "    \n",
    "    Also does hard negative mining. so requires generation of a selction mask of positives and most overconfident negatives.\n",
    "    \n",
    "    Args:\n",
    "        pred_class (b, 1, 60, 60): Network confidence probs\n",
    "        gt_class (b, 1, 60, 60): Binary gt confidence probs\n",
    "        \n",
    "    Returns:\n",
    "        class_loss: Scalar\n",
    "    \"\"\"\n",
    "    abs_loss = (pred_class - gt_class) ** 2\n",
    "    mask = selection_mask(abs_loss, gt_class)\n",
    "    selected_loss = abs_loss * mask\n",
    "    \n",
    "    return selection_mask.sum()/pred_class.shape[0]\n",
    "\n",
    "def regression_loss(pred_loc, gt_loc):\n",
    "    \"\"\"\n",
    "    Regression loss from vanilla mean squared diff between shifts.\n",
    "    \n",
    "    Args:\n",
    "        pred_loc (b, 4, 60, 60): Network offsets for top left and bottom right of box. Should \n",
    "        gt_loc (b, 4, 60, 60): Ground truth offsets for top left and bottom right of box.  \n",
    "        gt_class (b, 1, 60, 60): Offsets for positive examples\n",
    "        \n",
    "    Returns:\n",
    "        reg_loss: Scalar\n",
    "    \"\"\"\n",
    "    abs_loss = ((pred_loc - gt_loc) ** 2).sum(axis=1) # Check dims in test\n",
    "    selected_loss = abs_loss * gt_class\n",
    "    \n",
    "    return selected_loss.sum()/pred_loc.shape[0]\n",
    "\n",
    "def selection_mask(abs_loss, gt_class):\n",
    "    \"\"\"\n",
    "    Returns a binary mask from absolute mean square classification loss and the ground truth mask\n",
    "    \n",
    "    Args:\n",
    "        abs_loss (b, 1, 60, 60): Absolute probability loss value over each pixel\n",
    "        gt_class (b, 1, 60, 60): Binary gt probs to set the positive pixels to one\n",
    "        \n",
    "    Returns;\n",
    "        select_mask (b, 1, 60, 60): Selection mask for poth positive and negative pixels0\n",
    "    \"\"\"\n",
    "    yinv = 1 - gt_class\n",
    "    \n",
    "    l_neg = yinv*abs_class\n",
    "\n",
    "    select_mask = np.zeros(gt_class.shape)\n",
    "\n",
    "    for num,i in enumerate(l_neg):\n",
    "        indices = np.argsort(i.data,axis=None )\n",
    "\n",
    "        matrix_indices = np.unravel_index(indices,(3,3))\n",
    "        matrix_indices_flipped = np.fliplr(matrix_indices)\n",
    "\n",
    "        k_value= np.sum(y_star[num,:,:])\n",
    "       \n",
    "        k_value = int(k_value)\n",
    "\n",
    "        matrix_indices_flipped = matrix_indices_flipped[:,0:k_value]\n",
    "\n",
    "        select_mask[num,0,matrix_indices_flipped[0],matrix_indices_flipped[1]]= 1\n",
    "        \n",
    "        select_mask[num,0,:,:] =select_mask[num,0,:,:] + y_star[num,0,:,:]\n",
    "        \n",
    "    return select_mask \n",
    "\n",
    "class DenseBoxes(Chain):\n",
    "    def __init__(self):\n",
    "        super(DenseBoxes, self).__init__(\n",
    "        # List of all the layers in denseboxes here.\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests\n",
    "Writing tests for all of these methods. Checking all of them should be sufficient for this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lola\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.003s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "from unittest import TestSuite\n",
    "\n",
    "class TestCode(unittest.TestCase):\n",
    "    \n",
    "    def test_dataset(self):\n",
    "        \"\"\" Test that datast works \"\"\"\n",
    "        print('lola')\n",
    "        pass\n",
    "    \n",
    "    def test_encode_y(self):\n",
    "        \"\"\" LOGIC: Manually worked out example with batch size of two. Trivial category. \"\"\"\n",
    "        # Visualizing should also be fine\n",
    "        # Encoded boxes in a couple of boxes should look like the distance transform\n",
    "        print('lola')\n",
    "        \n",
    "    def test_downsample(self):\n",
    "        \"\"\" LOGIC: Test that shape matches. \"\"\"\n",
    "        # Visualizing should also be fine\n",
    "        pass\n",
    "    \n",
    "    def test_predict(self):\n",
    "        \"\"\" LOGIC: What does this method do? \"\"\"\n",
    "        # Visualizing should also be fine\n",
    "        pass\n",
    "    \n",
    "    def test_network(self):\n",
    "        \"\"\" LOGIC: Test that the shape is correct? \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def test_loss(self):\n",
    "        \"\"\" LOGIC: Calculate sample loss for the network. Trivial category. \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def test_reg_loss(self):\n",
    "        \"\"\" LOGIC: Calculate regression loss for the network with batch size of two. Trivial category. \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def test_class_loss(self):\n",
    "        \"\"\" LOGIC: Calculate classification loss for the network with batch size of two. Trivial category. \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def test_convergence(self):\n",
    "        \"\"\" Check that for the one and two squares datasets, the network loss convergence. Plot the loss and show that it converges. \"\"\"\n",
    "        \n",
    "ts = TestSuite()\n",
    "#ts.ad\n",
    "ts.addTests([TestCode('test_encode_y')])\n",
    "unittest.TextTestRunner().run(ts)\n",
    "#alltests = unittest.TestSuite([fast, slow])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
